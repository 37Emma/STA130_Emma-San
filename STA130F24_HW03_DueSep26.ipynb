{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae681ef",
   "metadata": {},
   "source": [
    "## STA130 Homework 03 \n",
    "\n",
    "Please see the course [wiki-textbook](https://github.com/pointOfive/stat130chat130/wiki) for the list of topics covered in this homework assignment, and a list of topics that might appear during ChatBot conversations which are \"out of scope\" for the purposes of this homework assignment (and hence can be safely ignored if encountered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf135c01",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Introduction</u></summary>\n",
    "\n",
    "### Introduction\n",
    "    \n",
    "A reasonable characterization of STA130 Homework is that it simply defines a weekly reading comprehension assignment. \n",
    "Indeed, STA130 Homework essentially boils down to completing various understanding confirmation exercises oriented around coding and writing tasks.\n",
    "However, rather than reading a textbook, STA130 Homework is based on ChatBots so students can interactively follow up to clarify questions or confusion that they may still have regarding learning objective assignments.\n",
    "\n",
    "> Communication is a fundamental skill underlying statistics and data science, so STA130 Homework based on ChatBots helps practice effective two-way communication as part of a \"realistic\" dialogue activity supporting underlying conceptual understanding building. \n",
    "\n",
    "It will likely become increasingly tempting to rely on ChatBots to \"do the work for you\". But when you find yourself frustrated with a ChatBots inability to give you the results you're looking for, this is a \"hint\" that you've become overreliant on the ChatBots. Your objective should not be to have ChatBots \"do the work for you\", but to use ChatBots to help you build your understanding so you can efficiently leverage ChatBots (and other resources) to help you work more efficiently.<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Instructions</u></summary>\n",
    "\n",
    "### Instructions\n",
    "    \n",
    "1. Code and write all your answers (for both the \"Pre-lecture\" and \"Post-lecture\" HW) in a python notebook (in code and markdown cells) \n",
    "    \n",
    "> It is *suggested but not mandatory* that you complete the \"Pre-lecture\" HW prior to the Monday LEC since (a) all HW is due at the same time; but, (b) completing some of the HW early will mean better readiness for LEC and less of a \"procrastentation cruch\" towards the end of the week...\n",
    "    \n",
    "2. Paste summaries of your ChatBot sessions (including link(s) to chat log histories if you're using ChatGPT) within your notebook\n",
    "    \n",
    "> Create summaries of your ChatBot sessions by using concluding prompts such as \"Please provide a summary of our exchanges here so I can submit them as a record of our interactions as part of a homework assignment\" or, \"Please provide me with the final working verson of the code that we created together\"\n",
    "    \n",
    "3. Save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Prompt Engineering?</u></summary>\n",
    "    \n",
    "### Prompt Engineering? \n",
    "    \n",
    "The questions (as copy-pasted prompts) are designed to initialize appropriate ChatBot conversations which can be explored in the manner of an interactive and dynamic textbook; but, it is nonetheless **strongly recommendated** that your rephrase the questions in a way that you find natural to ensure a clear understanding of the question. Given sensible prompts the represent a question well, the two primary challenges observed to arise from ChatBots are \n",
    "\n",
    "1. conversations going beyond the intended scope of the material addressed by the question; and, \n",
    "2. unrecoverable confusion as a result of sequential layers logial inquiry that cannot be resolved. \n",
    "\n",
    "In the case of the former (1), adding constraints specifying the limits of considerations of interest tends to be helpful; whereas, the latter (2) is often the result of initial prompting that leads to poor developments in navigating the material, which are likely just best resolve by a \"hard reset\" with a new initial approach to prompting.  Indeed, this is exactly the behavior [hardcoded into copilot](https://answers.microsoft.com/en-us/bing/forum/all/is-this-even-normal/0b6dcab3-7d6c-4373-8efe-d74158af3c00)...\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7075ccb5",
   "metadata": {},
   "source": [
    "\n",
    "### Marking Rubric (which may award partial credit) \n",
    "\n",
    "- [0.1 points]: All relevant ChatBot summaries [including link(s) to chat log histories if you're using ChatGPT] are reported within the notebook\n",
    "- [0.2 points]: Assignment completion confirmed by visual submission for \"2\" \n",
    "- [0.3 points]: Evaluation of written communication for \"3\" \n",
    "- [0.1 points]: Correct answers for \"4\"\n",
    "- [0.3 points]: Evidence of meaningful activity for \"6\"\n",
    "\n",
    "<!-- - [0.1 points]: Assignment completion confirmed by ChatBot interaction summaries for \"5\" -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a945f7c",
   "metadata": {},
   "source": [
    "### \"Pre-lecture\" HW [*completion prior to next LEC is suggested but not mandatory*]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e78c77",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. Use *fig.add_[h/v]line()* and *fig.add_[h/v]rect()* to mark, respspectively, location (mean and median) and scale (range, interquartile range, and a range defined by two standard deviations away from the mean in both directions) of *flipper_length_mm* for each _species_ onto _plotly_ histograms of *flipper_length_mm* for each _species_ in the penguins dataset<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "**Time Management Warning**: it takes a long time to make a figure, whether you're working with a ChatBot, or building it from scratch based on trial and error changes with your code. ChatBots remove the need to understand the detailed nuances of data visualization library arguments and construction procedures. But after you've passed the 30 minute range of effort working with your ChatBot for this problem to try to get what you want, then your only options are to start a new session and hope for a smoother experience based on improved clarity of your directions, or submit what you have along with a brief note highlighting the duration in your chatlog history where your efforts to make progress did not produce the desired outcome.\n",
    "\n",
    "> The code referenced above [*fig.add_[h/v]line()*](https://plotly.com/python/horizontal-vertical-shapes/) and [*fig.add_[h/v]rect()*](https://plotly.com/python/line-charts/) refer to `fig.add_hline()` and `fig.add_hline()` and `fig.add_hrect()` and `fig.add_vrect()` which overly lines rectangles onto a figure from different orientation perspectives.\n",
    ">\n",
    "> - _There are several considerations in this problem..._\n",
    ">     - _The histograms can be on the same figure, on separate figures, or separated into different panels in the same figure_\n",
    ">     - _The elements within a figure should be well annotated, probobably using a so-called legend to help make sure annotations don't overlap each other and are clear and readible_\n",
    "> - _There are several ways to approach this problem..._\n",
    ">     - _You will likely be very pleased when you run the code returned to you as the result of pasting this question in as a prompt into a ChatBot session; but, you will also likely need to interact with the ChatBot to ask for adjustments to the code which give a final satisfactory figure (and this is the recommended approach to get the experience this problem intends you to have)_\n",
    ">     - _**When using a ChatBot, if the code provided by your ChatBot results in an error, show the error to your ChatBot and iterate this process with the adjusted \"fixed\" code provided by the ChatBot... this process usually converges some something workable that's pretty close to what you were going for**_\n",
    ">     - <u>**And don't forget, a ChatBot can explain what how code it provides works, if you ask it to...**</u>\n",
    ">     - _You could alternatively figure out how to code this plot up for yourself by looking at the provided documentation links and perhaps using some additional google searchers or ChatBot queries to help out with specific issues or examples; and, if you end up interested in figuring out a little more how the code works that's great and definitely feel free to go ahead and do so, but at this stage the point of this problem is to understand the general ideas of figures themselves as opposed to being an expert about the code that generated them_\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e01df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the penguins dataset\n",
    "penguins = sns.load_dataset('penguins')\n",
    "\n",
    "# Drop rows with missing flipper_length_mm values\n",
    "penguins = penguins.dropna(subset=['flipper_length_mm'])\n",
    "\n",
    "# Create a histogram for each species\n",
    "species = penguins['species'].unique()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for sp in species:\n",
    "    # Filter data by species\n",
    "    data = penguins[penguins['species'] == sp]\n",
    "    flipper_lengths = data['flipper_length_mm']\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean = np.mean(flipper_lengths)\n",
    "    median = np.median(flipper_lengths)\n",
    "    std = np.std(flipper_lengths)\n",
    "    min_val = np.min(flipper_lengths)\n",
    "    max_val = np.max(flipper_lengths)\n",
    "    q1 = np.percentile(flipper_lengths, 25)\n",
    "    q3 = np.percentile(flipper_lengths, 75)\n",
    "    \n",
    "    # Add histogram for the species\n",
    "    fig.add_trace(go.Histogram(\n",
    "        x=flipper_lengths,\n",
    "        name=f'{sp} flipper length',\n",
    "        opacity=0.5\n",
    "    ))\n",
    "    \n",
    "    # Add lines for mean and median\n",
    "    fig.add_vline(x=mean, line=dict(color='blue', width=2, dash='dash'), annotation_text=f'{sp} mean', annotation_position=\"top left\")\n",
    "    fig.add_vline(x=median, line=dict(color='green', width=2, dash='dot'), annotation_text=f'{sp} median', annotation_position=\"top left\")\n",
    "    \n",
    "    # Add rectangles for range, IQR, and 2*std from mean\n",
    "    fig.add_vrect(x0=min_val, x1=max_val, fillcolor=\"lightgrey\", opacity=0.2, line_width=0, annotation_text=f'{sp} range')\n",
    "    fig.add_vrect(x0=q1, x1=q3, fillcolor=\"orange\", opacity=0.3, line_width=0, annotation_text=f'{sp} IQR')\n",
    "    fig.add_vrect(x0=mean - 2*std, x1=mean + 2*std, fillcolor=\"lightblue\", opacity=0.2, line_width=0, annotation_text=f'{sp} ±2 std dev')\n",
    "    \n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Flipper Length Distributions with Mean, Median, Range, IQR, and Std Dev for Each Species\",\n",
    "    xaxis_title=\"Flipper Length (mm)\",\n",
    "    yaxis_title=\"Count\",\n",
    "    barmode='overlay'\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11cea10",
   "metadata": {},
   "source": [
    "To mark the locations (mean and median) and scale (range, interquartile range, and a range defined by two standard deviations away from the mean) of flipper_length_mm for each species in the penguins dataset onto Plotly histograms, we will use the following steps:\n",
    "\n",
    "1. Plotly histogram: Create histograms for the flipper_length_mm for each species.\n",
    "2. Mean and Median: Use fig.add_vline() to mark the mean and median.\n",
    "3. Range, IQR, and Std. Dev Range: Use fig.add_vrect() to mark:\n",
    "    The full range (min to max).\n",
    "    The interquartile range (IQR).\n",
    "    The range within two standard deviations from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29540adc",
   "metadata": {},
   "source": [
    "#### 2. Transition your ChatBot session from the previous problem to repeat the previous problem, but this time using _seaborn_ **kernel density estimation** (KDE) plots to produce the desired figures organized in row of three plots<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "**Time Management Warning**: it takes a long time to make a figure, whether you're working with a ChatBot, or building it from scratch based on trial and error changes with your code. ChatBots remove the need to understand the detailed nuances of data visualization library arguments and construction procedures. But after you've passed the 30 minute range of effort working with your ChatBot for this problem to try to get what you want, then your only options are to start a new session and hope for a smoother experience based on improved clarity of your directions, or submit what you have along with a brief note highlighting the duration in your chatlog history where your efforts to make progress did not produce the desired outcome.\n",
    "    \n",
    "> The `seaborn` library extends `matplotlib` so [_ax.axhspan(...)_](https://matplotlib.org/stable/gallery/subplots_axes_and_figures/axhspan_demo.html#sphx-glr-gallery-subplots-axes-and-figures-axhspan-demo-py) or [_ax.fill_between(...)_](https://matplotlib.org/stable/gallery/lines_bars_and_markers/span_regions.html) from `matplotlib` could be combined with the [_seaborn_ KDE plot](https://seaborn.pydata.org/generated/seaborn.kdeplot.html)... this might be something to share with your ChatBot if it [tries to keep using _plotly_ or a KDE function rather than a _plotly_](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk3/GPT/SLS/00001_gpt3p5_plotlyseaborn_plotting.md) plotting functionality...\n",
    "> \n",
    "> - _When using a ChatBot, if the code provided by your ChatBot results in an error, show the error to your ChatBot and iterate this process with the adjusted \"fixed\" code provided by the ChatBot... this process usually converges some something workable that's pretty close to what you were going for_\n",
    "> - _**Also consider the ways that you might be able to split up the instructions for the ChatBot into multiple steps, creating a sequence of additional directions and extensions along the way as you mold the figure more and more into a form increasingly matching your desired output.**_\n",
    "> - And don't forget, a ChatBot can explain what how code it provides works, if you ask it to...\n",
    "> \n",
    "> The technical details of the following are beyond the scope of STA130, but if you were interested, you could very briefly examine the [_seaborn_ themes](https://seaborn.pydata.org/tutorial/aesthetics.html) based on `sns.set_style()` and `sns.set_theme()` and [_colors_](https://seaborn.pydata.org/tutorial/color_palettes.html) based on the `palette` parameter, e.g.,\n",
    "> \n",
    "> ```python\n",
    "> sns.set_style(\"whitegrid\") # sns.set_style(\"dark\")\n",
    "> # `sns.set_palette()` exists but functions often access and set that directly\n",
    "> sns.boxplot(..., hue='column', palette=\"colorblind\") \n",
    "> ```    \n",
    "> \n",
    "> and then attempt to interact with the ChatBot to change the coloring of the figure to something that you like and looks more clear to you... \n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9436b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the penguins dataset\n",
    "penguins = sns.load_dataset('penguins')\n",
    "\n",
    "# Drop rows with missing flipper_length_mm values\n",
    "penguins = penguins.dropna(subset=['flipper_length_mm'])\n",
    "\n",
    "# Create a list of species\n",
    "species = penguins['species'].unique()\n",
    "\n",
    "# Create a 1x3 grid for the KDE plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# Loop through each species and create KDE plots\n",
    "for i, sp in enumerate(species):\n",
    "    # Filter data by species\n",
    "    data = penguins[penguins['species'] == sp]\n",
    "    flipper_lengths = data['flipper_length_mm']\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean = np.mean(flipper_lengths)\n",
    "    median = np.median(flipper_lengths)\n",
    "    std = np.std(flipper_lengths)\n",
    "    min_val = np.min(flipper_lengths)\n",
    "    max_val = np.max(flipper_lengths)\n",
    "    q1 = np.percentile(flipper_lengths, 25)\n",
    "    q3 = np.percentile(flipper_lengths, 75)\n",
    "    \n",
    "    # Plot KDE\n",
    "    sns.kdeplot(flipper_lengths, ax=axes[i], fill=True, label=f'{sp} KDE', color='skyblue')\n",
    "    \n",
    "    # Add lines for mean and median\n",
    "    axes[i].axvline(mean, color='blue', linestyle='--', label=f'{sp} mean')\n",
    "    axes[i].axvline(median, color='green', linestyle='-.', label=f'{sp} median')\n",
    "    \n",
    "    # Add rectangles for range, IQR, and 2*std from mean\n",
    "    axes[i].axvspan(min_val, max_val, color='lightgrey', alpha=0.3, label=f'{sp} range')\n",
    "    axes[i].axvspan(q1, q3, color='orange', alpha=0.3, label=f'{sp} IQR')\n",
    "    axes[i].axvspan(mean - 2*std, mean + 2*std, color='lightblue', alpha=0.2, label=f'{sp} ±2 std dev')\n",
    "    \n",
    "    # Set title and labels\n",
    "    axes[i].set_title(f'{sp} Flipper Length KDE')\n",
    "    axes[i].set_xlabel('Flipper Length (mm)')\n",
    "    if i == 0:\n",
    "        axes[i].set_ylabel('Density')\n",
    "    \n",
    "    # Display legends for each plot\n",
    "    axes[i].legend(loc='upper left')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e872343c",
   "metadata": {},
   "source": [
    "Goal: Create Seaborn kernel density estimation (KDE) plots of flipper_length_mm for each penguin species, overlaid with statistical markers (mean, median, range, interquartile range (IQR), and ±2 standard deviations from the mean).\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. KDE Plots: Generated one KDE plot for each species using seaborn.kdeplot(), with a filled density curve.\n",
    "2. Statistical Markers:\n",
    "    Mean and Median: Added vertical dashed and dash-dot lines at the mean and median, respectively, using axvline().\n",
    "    Range, IQR, ±2 Std Dev: Used shaded areas with axvspan() to mark the full range, IQR, and two standard deviations away from the mean.\n",
    "3. Figure Layout: The plots are organized in a row of three subplots (one per species), with shared y-axis and separate legends showing the statistical information.\n",
    "\n",
    "Outcome: The final output visually shows the distribution of flipper lengths with important statistical information for each penguin species in a row of KDE plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab89605",
   "metadata": {},
   "source": [
    "#### 3. Search online for some images of **box plots**, **histograms**, and **kernel density estimators** (perhaps for the same data set); describe to a ChatBot what you think the contrasting descriptions of these three \"data distribution\" visualization methods are; and then see if the ChatBot agrees and what \"pros and cons\" list of these three \"data distribution\" visualization methods your ChatBot can come up with; finally, describe your preference for one or the other and your rationale for this preference<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> The details of the [\"kernel\"](https://en.wikipedia.org/wiki/Kernel_density_estimation) and how it works in [kernel density estimation](https://plotly.com/python/violin/#split-violin-plot) are beyond the scope of STA130; but, there is typically a so-called \"bandwidth\" **argument** (e.g., `bw_adjust` in [_seaborn_](https://stackoverflow.com/questions/37932283/confusion-with-bandwidth-on-seaborns-kdeplot)) that \"controls the width of the kernel\" which is analgous to the \"number of bins parameter\" of a histogram (e.g., `nbins` in [_plotly_](https://www.google.com/search?client=safari&rls=en&q=plotly+nbins&ie=UTF-8&oe=UTF-8))  <!-- 4. Report on your preferences between `plotly` and `seaborn` in terms of usability and the general visual aestetics -->\n",
    "> \n",
    "> _Don't forget to ask for summaries of your ChatBot session(s) and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatGPT)_\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf8473",
   "metadata": {},
   "source": [
    "### Histogram\n",
    "Pros: Simple, intuitive, works well with large datasets.\n",
    "Cons: Sensitive to bin size, can miss details of distribution structure depending on binning choices\n",
    "\n",
    "### Kernel Density Estimates (KDEs)\n",
    "Pros: Smooth, visually appealing, better for understanding continuous data.\n",
    "Cons: Bandwidth choice is subjective and can affect the shape of the curve. Misleading if bandwidth is too large or too small\n",
    "\n",
    "### Box Plots\n",
    "Pros: Compact, summarizes distribution with clear indication of outliers, good for comparisons.\n",
    "Cons: Doesn’t show multimodality or the full distribution shape\n",
    "\n",
    "### Preference\n",
    "If your goal is a quick comparison of the spread and central tendency, box plots work well. For understanding the full shape of a distribution, KDEs are preferable since they give a continuous estimate of the data. My personal preference leans towards KDEs because they offer a more detailed and nuanced view of the data without the binning issues of histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8cc949",
   "metadata": {},
   "source": [
    "#### 4. Run the code below and look at the resulting figure of distrubutions and then answer the following questions\n",
    "\n",
    "1. Which datasets have similar means and similar variances\n",
    "2. Which datasets have similar means but quite different variances\n",
    "3. Which datasets have similar variances but quite different means\n",
    "4. Which datasets have quite different means and quite different variances\n",
    "    \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> Can you answer these questions immediately? If not, first review what the basic ideas of **sample means** and **sample standard deviations** (and **sample variances**) are. Their mathematical definitions are given below, and are useful for understanding the intuition of these concepts in terms of \"averages\" of things, like \"observations\" or \"squared differences\" (and then perhaps square-rooted). But there are other ways to \"intuitively visually\" understand **sample means** and **sample standard deviations** (and **sample variances**) which a ChatBot would be able to discuss with you.\n",
    ">\n",
    "> - sample mean $\\displaystyle \\bar x = \\frac{1}{n}\\sum_{i=1}^n x_i$ \n",
    "> - sample variance $\\displaystyle s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar x)^2$\n",
    "> - sample standard deviation $\\displaystyle s = \\sqrt{s^2}$\n",
    ">\n",
    "> It's potentially maybe possible that you or a ChatBot could answer these questions by looking at the code that produced the data you're considering. But if you're trying to check and understand things that way, you should instead consider just calculate the statistics that answer the questions themselves...\n",
    "> - `np.mean(df.col)` or `df.col.mean()`\n",
    "> - `np.std(df.col, dof=1)` / `np.var(df.col, dof=1)` or `df.col.std(dof=1)` / `df.col.var(dof=1)`\n",
    ">\n",
    "> _If you are resorting to calculating the statistics that answer the questions, try to understand the answers after you have them... just getting the \"right\" answers kind of defeats the point of this exercise..._\n",
    ">\n",
    "> - The difference between trying to answer this question using the code that produced the data versus calculating the statistics from the data comes down to the difference between **parameters** and **statistics**, but this will be discussed in the lecture... in the meantime, howevever, if you're curious about this... you could consider prompting a ChatBot to explain the difference between **parameters** and **statistics**...\n",
    ">     - ... this would naturally lead to some discussion of the relationship between **populations** and **samples**, and from there it would only be a little further to start working to understand the relationship between **statistics** and **parameters** and how they connect to *populations* and *samples* (and hence each other)...    \n",
    "    \n",
    "</details>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c149e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "n = 1500\n",
    "data1 = stats.uniform.rvs(0, 10, size=n)\n",
    "data2 = stats.norm.rvs(5, 1.5, size=n)\n",
    "data3 = np.r_[stats.norm.rvs(2, 0.25, size=int(n/2)), stats.norm.rvs(8, 0.5, size=int(n/2))]\n",
    "data4 = stats.norm.rvs(6, 0.5, size=n)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=4)\n",
    "\n",
    "fig.add_trace(go.Histogram(x=data1, name='A', nbinsx=30, marker=dict(line=dict(color='black', width=1))), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=data2, name='B', nbinsx=15, marker=dict(line=dict(color='black', width=1))), row=1, col=2)\n",
    "fig.add_trace(go.Histogram(x=data3, name='C', nbinsx=45, marker=dict(line=dict(color='black', width=1))), row=1, col=3)\n",
    "fig.add_trace(go.Histogram(x=data4, name='D', nbinsx=15, marker=dict(line=dict(color='black', width=1))), row=1, col=4)\n",
    "\n",
    "fig.update_layout(height=300, width=750, title_text=\"Row of Histograms\")\n",
    "fig.update_xaxes(title_text=\"A\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"B\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"C\", row=1, col=3)\n",
    "fig.update_xaxes(title_text=\"D\", row=1, col=4)\n",
    "fig.update_xaxes(range=[-0.5, 10.5])\n",
    "\n",
    "for trace in fig.data:\n",
    "    trace.xbins = dict(start=0, end=10)\n",
    "    \n",
    "# This code was produced by just making requests to Microsoft Copilot\n",
    "# https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk3/COP/SLS/0001_concise_makeAplotV1.md\n",
    "\n",
    "fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b57062f",
   "metadata": {},
   "source": [
    "1. Similar means: data1, data2, and data3 all have means close to 5, making their central tendencies similar.\n",
    "2. Similar variances:\n",
    "    data1 and data3 have higher and similar variances (~8.33 and ~9.5).\n",
    "    data2 has a lower variance (~2.25) and is distinct from data1 and data3.\n",
    "    data4 has a small variance (~0.25), indicating less spread around its mean compared to the others.\n",
    "\n",
    "Thus, data1, data2, and data3 have similar means, but only data1 and data3 have comparable variances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24abf40e",
   "metadata": {},
   "source": [
    "Similar means but different variances:\n",
    "data1, data2, and data3 have very similar means (~5.0).\n",
    "\n",
    "However, their variances are quite different:\n",
    "\n",
    "data1 (Uniform): Variance ~8.33\n",
    "data2 (Normal): Variance ~2.25\n",
    "data3 (Bimodal): Variance ~9.5\n",
    "Comparison:\n",
    "\n",
    "data1 and data3 have high variances (~8.33 and ~9.5).\n",
    "data2 has a much lower variance (~2.25).\n",
    "Thus, data1, data2, and data3 have similar means (close to 5) but quite different variances, with data1 and data3 having much larger variances compared to data2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e89f4f",
   "metadata": {},
   "source": [
    "Similar variances but different means:\n",
    "data2 (variance ~2.25) and data4 (variance ~0.25) have quite different means:\n",
    "\n",
    "data2 mean: ~5.0\n",
    "data4 mean: ~6.0\n",
    "However, their variances differ significantly, with data4 having a very small variance compared to data2.\n",
    "\n",
    "Thus, no datasets in this example have similar variances but quite different means. Most datasets with similar variances tend to have close means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0481ad",
   "metadata": {},
   "source": [
    "Quite different means and quite different variances:\n",
    "data4 (Normal, μ=6) vs data1 (Uniform, μ=5) or data3 (Bimodal, μ=5):\n",
    "Means: data4 has a mean of ~6.0, which is significantly different from the mean of ~5.0 for data1 and data3.\n",
    "Variances: data4 has a very small variance (~0.25), while data1 (~8.33) and data3 (~9.5) have large variances.\n",
    "Thus, data4 (Normal, μ=6, variance ~0.25) has quite different means and variances compared to both data1 (Uniform, μ=5, variance ~8.33) and data3 (Bimodal, μ=5, variance ~9.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6279b8",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Continue now...?</u></summary>\n",
    "\n",
    "### Pre-lecture VS Post-lecture HW\n",
    "\n",
    "Feel free to work on the \"Post-lecture\" HW below if you're making good progress and want to continue: the next questions will just continue working on data visualization related topics, so, it's just a choice whether or not you want to work a head a little bit... \n",
    "\n",
    "- The previous suggestions regarding **parameters** versus **statistics** would be a very good thing to look at carefully in preparation for the upcoming lecture...\n",
    "    \n",
    "*The benefits of continue would are that (a) it might be fun to try to tackle the challenge of working through some problems without additional preparation or guidance; and (b) this is a very valable skill to be comfortable with; and (c) it will let you build experience interacting with ChatBots (and beginning to understand their strengths and limitations in this regard)... it's good to have sense of when using a ChatBot is the best way to figure something out, or if another approach (such as course provided resources or a plain old websearch for the right resourse) would be more effective*\n",
    "    \n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99c427",
   "metadata": {},
   "source": [
    "### \"Post-lecture\" HW [*submission along with \"Pre-lecture\" HW is due prior to next TUT*]\n",
    "\n",
    "#### 5. Start a new ChatBot session to explore the general relationship between the *mean* and *median* and \"right\" and \"left\" skewness (and why this is); what the following code does and how it works; and then explain (in your own words) the relationship between the *mean* and *median* and \"right\" and \"left\" skewness and what causes this, using and extending the code to demonstrate your explanation through a sequence of notebook cells.<br>\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "  \n",
    "sample1 = stats.gamma(a=2,scale=2).rvs(size=1000)\n",
    "fig1 = px.histogram(pd.DataFrame({'data': sample1}), x=\"data\")\n",
    "# USE `fig1.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS\n",
    "\n",
    "sample1.mean()\n",
    "np.quantile(sample1, [0.5]) # median\n",
    "\n",
    "sample2 = -stats.gamma(a=2,scale=2).rvs(size=1000)\n",
    "```\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> You could start this session perhaps something like [this](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk3/GPT/SLS/00003_GPT3p5_meanVmedian.md)?\n",
    "> \n",
    "> _Don't forget to ask for summaries of your ChatBot session(s) and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatGPT)..._\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcaf83d",
   "metadata": {},
   "source": [
    "Right-Skewed Distribution:\n",
    "\n",
    "After generating and analyzing the right-skewed distribution, you should find that the mean is greater than the median. This confirms the property of right skewness.\n",
    "Left-Skewed Distribution:\n",
    "\n",
    "For the left-skewed distribution, the mean will typically be less than the median, demonstrating the property of left skewness.\n",
    "\n",
    "Conclusion\n",
    "By generating both right and left skewed distributions and calculating their means and medians, you can visually and quantitatively observe the relationship between these statistics and skewness. This approach illustrates how outliers in either direction (high for right skew, low for left skew) affect the mean more significantly than the median, resulting in a clear skewness in the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff445fc",
   "metadata": {},
   "source": [
    "#### 6. Go find an interesting dataset and use summary statistics and visualizations to understand and demonstate some interesting aspects of the data<br>\n",
    "\n",
    "1. Your approach should likely follow what was suggested for the **Week 02 TUT Communication Activity from TUT**\n",
    "2. In the **Week 03 TUT Communication Activity from TUT** you will be put in groups and determine which group members dataset introduction will be presented by the group\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> A good place to browse datasets is [TidyTuesday](https://github.com/rfordatascience/tidytuesday/blob/master/README.md) as working with ChatBots to find unconventional and entertaining datasets is not particularly productive and only seems to end up with the datasets seen here and other (more interesting?) suggestions like [iris](https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv), [superheros](https://raw.githubusercontent.com/steview-d/superhero-dashboard/master/static/data/superheroData.csv), [hauntings](https://raw.githubusercontent.com/andreamoleri/Hauntings/main/hauntings.csv), [bigfoot](https://raw.githubusercontent.com/hannahramirez/BigfootVsUfos/main/bigfoot_mod.csv), [ufos](https://raw.githubusercontent.com/hannahramirez/BigfootVsUfos/main/ufo_mod.csv), [sharks](https://raw.githubusercontent.com/IbaiGallego/DataCleaning_SharkAttack/main/data/jaws.csv), [legos](https://raw.githubusercontent.com/seankross/lego/master/data-tidy/legosets.csv), [bees](https://gist.githubusercontent.com/bootshine2/ba15d3cb38e2ed31129aeca403405a12/raw/10949901cd8a6a75aa46c86b804c42ff410f929e/Bee%2520Colony%2520Loss.csv), [housing](https://raw.githubusercontent.com/slavaspirin/Toronto-housing-price-prediction/master/houses_edited.csv), and [gapminder](https://raw.githubusercontent.com/kirenz/datasets/master/gapminder.csv)\n",
    "> ```python\n",
    "> # Maybe something like this? Feel free to use this one \n",
    "> # if it strikes your fancy after look around a bit\n",
    "> import pandas as pd\n",
    "> df = pd.read_csv(\"https://raw.githubusercontent.com/manuelamc14/fast-food-Nutritional-Database/main/Tables/nutrition.csv\")\n",
    "> df # df.columns\n",
    "> ```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e56fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Iris dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "columns = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
    "iris_data = pd.read_csv(url, header=None, names=columns)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "iris_data.head()\n",
    "# Summary statistics\n",
    "summary_stats = iris_data.describe()\n",
    "summary_stats\n",
    "import plotly.express as px\n",
    "\n",
    "# Plot histograms for each feature\n",
    "fig = px.histogram(iris_data, x='SepalLength', color='Species', title='Sepal Length Distribution')\n",
    "fig.show(renderer=\"png\")\n",
    "\n",
    "fig = px.histogram(iris_data, x='SepalWidth', color='Species', title='Sepal Width Distribution')\n",
    "fig.show(renderer=\"png\")\n",
    "\n",
    "fig = px.histogram(iris_data, x='PetalLength', color='Species', title='Petal Length Distribution')\n",
    "fig.show(renderer=\"png\")\n",
    "\n",
    "fig = px.histogram(iris_data, x='PetalWidth', color='Species', title='Petal Width Distribution')\n",
    "fig.show(renderer=\"png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816717a6",
   "metadata": {},
   "source": [
    "Observations and Insights\n",
    "Summary Statistics: The summary statistics will provide insights into the mean, standard deviation, min, and max for each feature, allowing for a quick assessment of the dataset.\n",
    "\n",
    "Histograms: The histograms will show the distribution of each feature for different species, highlighting any overlapping or distinct ranges.\n",
    "\n",
    "Box Plots: These will indicate the central tendency and spread of the data, as well as potential outliers, for each feature by species.\n",
    "\n",
    "Pair Plot: This plot will provide insights into the relationships between pairs of features, showing how well the different species can be separated based on their measurements.\n",
    "\n",
    "Violin Plots: Similar to box plots, but they also show the kernel density estimation, providing a more comprehensive view of the distribution of the data.\n",
    "\n",
    "Conclusion\n",
    "Through the summary statistics and visualizations, you can uncover interesting aspects of the Iris dataset, such as how distinct the species are based on their measurements and which features contribute the most to their differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f2749",
   "metadata": {},
   "source": [
    "#### 7. Watch the classic [Gapminder Video](https://www.youtube.com/watch?v=jbkSRLYSojo), then have a look at the [`plotly` version](https://plotly.com/python/animations/) and recreate the animation (perhaps after optionally exploring and changing the [style](https://plotly.com/python/templates/), if you wish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff2d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Gapminder dataset\n",
    "url = \"https://raw.githubusercontent.com/plotly/datasets/master/gapminderDataFiveYear.csv\"\n",
    "gapminder = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "gapminder.head()\n",
    "import plotly.express as px\n",
    "\n",
    "# Create an animated scatter plot\n",
    "fig = px.scatter(gapminder,\n",
    "                 x='gdpPercap',\n",
    "                 y='lifeExp',\n",
    "                 animation_frame='year',\n",
    "                 animation_group='country',\n",
    "                 size='pop',\n",
    "                 color='continent',\n",
    "                 hover_name='country',\n",
    "                 log_x=True,\n",
    "                 size_max=60,\n",
    "                 title='Gapminder: GDP per Capita vs Life Expectancy',\n",
    "                 labels={'gdpPercap': 'GDP per Capita', 'lifeExp': 'Life Expectancy'})\n",
    "\n",
    "# Show the animated plot\n",
    "fig.show(renderer=\"png\")\n",
    "# Update the layout for better aesthetics\n",
    "fig.update_layout(\n",
    "    title='Gapminder: GDP per Capita vs Life Expectancy',\n",
    "    xaxis_title='GDP per Capita (log scale)',\n",
    "    yaxis_title='Life Expectancy',\n",
    "    legend_title='Continent',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Show the styled animated plot\n",
    "fig.show(renderer=\"png\")\n",
    "# Change the size variable to 'gdpPercap' to see how GDP affects point size\n",
    "fig = px.scatter(gapminder,\n",
    "                 x='gdpPercap',\n",
    "                 y='lifeExp',\n",
    "                 animation_frame='year',\n",
    "                 animation_group='country',\n",
    "                 size='gdpPercap',\n",
    "                 color='continent',\n",
    "                 hover_name='country',\n",
    "                 log_x=True,\n",
    "                 size_max=60)\n",
    "# Add annotations to highlight specific countries\n",
    "fig.add_annotation(x=5000, y=80, text='Example Annotation', showarrow=True, arrowhead=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2be820",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "Creating an animated scatter plot with the Gapminder dataset using Plotly provides a dynamic way to visualize changes in global development indicators over time. You can further explore the dataset by changing the visual aspects to highlight different insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c953507",
   "metadata": {},
   "source": [
    "#### 8. Provide a second version of the figure from the previous problem where you edit the `fig = px.scatter()` function from the Gapminder code so that `x` is \"percent change\", `y` is \"rank\", `size` is \"percent\", and `color`=\"sex\", `animation_frame` is \"year\", and `animation_group` and `hover_name` are \"name\". Then use `size_max=50`, `range_x=[-0.005,0.005])` and remove the `log_x=True` and `range_y` parameters\n",
    "\n",
    "> ```python\n",
    "> bn = pd.read_csv('https://raw.githubusercontent.com/hadley/data-baby-names/master/baby-names.csv')\n",
    "> bn['name'] = bn['name']+\" \"+bn['sex'] # make identical boy and girl names distinct\n",
    "> bn['rank'] = bn.groupby('year')['percent'].rank(ascending=False)\n",
    "> bn = bn.sort_values(['name','year'])\n",
    "> # the next three lines create the increaes or decrease in name prevalence from the last year \n",
    "> bn['percent change'] = bn['percent'].diff()\n",
    "> new_name = [True]+list(bn.name[:-1].values!=bn.name[1:].values)\n",
    "> bn.loc[new_name,'percentage change'] = bn.loc[new_name,'percent'] \n",
    "> bn = bn.sort_values('year')\n",
    "> bn = bn[bn.percent>0.001] # restrict to \"common\" names\n",
    "> fig = px.scatter(bn, x=\"\", y=\"\", animation_frame=\"\", animation_group=\"\",\n",
    ">                  size=\"\", color=\"\", hover_name=\"\",size_max=50, range_x=[-0.005,0.005]) # range_y removed\n",
    "> fig.update_yaxes(autorange='reversed') # this lets us put rank 1 on the top\n",
    "> fig.show(renderer=\"png\") # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS\n",
    "> ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Load the dataset\n",
    "bn = pd.read_csv('https://raw.githubusercontent.com/hadley/data-baby-names/master/baby-names.csv')\n",
    "\n",
    "# Make identical boy and girl names distinct\n",
    "bn['name'] = bn['name'] + \" \" + bn['sex']  \n",
    "bn['rank'] = bn.groupby('year')['percent'].rank(ascending=False)\n",
    "bn = bn.sort_values(['name', 'year'])\n",
    "\n",
    "# Calculate percent change\n",
    "bn['percent change'] = bn['percent'].diff()\n",
    "new_name = [True] + list(bn.name[:-1].values != bn.name[1:].values)\n",
    "bn.loc[new_name, 'percent change'] = bn.loc[new_name, 'percent'] \n",
    "bn = bn.sort_values('year')\n",
    "\n",
    "# Restrict to \"common\" names\n",
    "bn = bn[bn.percent > 0.001] \n",
    "\n",
    "# Create the modified scatter plot\n",
    "fig = px.scatter(bn, \n",
    "                 x='percent change', \n",
    "                 y='rank', \n",
    "                 animation_frame='year', \n",
    "                 animation_group='name', \n",
    "                 size='percent', \n",
    "                 color='sex', \n",
    "                 hover_name='name', \n",
    "                 size_max=50, \n",
    "                 range_x=[-0.005, 0.005]) \n",
    "\n",
    "fig.update_yaxes(autorange='reversed')  # Reverse the y-axis to put rank 1 on top\n",
    "fig.update_layout(title='Baby Names: Percent Change vs Rank Over Time',\n",
    "                  xaxis_title='Percent Change',\n",
    "                  yaxis_title='Rank',\n",
    "                  template='plotly_white')\n",
    "\n",
    "# Show the plot\n",
    "fig.show(renderer=\"png\")  # For GitHub and MarkUs submissions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa20ebc",
   "metadata": {},
   "source": [
    "This code will produce an animated scatter plot that captures how the prevalence of baby names changes over time, along with their ranks and distinctions between genders. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe842f2",
   "metadata": {},
   "source": [
    "#### 9. Have you reviewed the course wiki-textbook and interacted with a ChatBot (or, if that wasn't sufficient, real people in the course piazza discussion board or TA office hours) to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it?<br><br>\n",
    "  \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    ">  Here is the link of [wiki-textbook](https://github.com/pointOfive/stat130chat130/wiki) in case it gets lost among all the information you need to keep track of  : )\n",
    ">     \n",
    "> _Just answering \"Yes\" or \"No\" or \"Somewhat\" or \"Mostly\" or whatever here is fine as this question isn't a part of the rubric; but, the midterm and final exams may ask questions that are based on the tutorial and lecture materials; and, your own skills will be limited by your familiarity with these materials (which will determine your ability to actually do actual things effectively with these skills... like the course project...)_\n",
    "    \n",
    "</details>\n",
    "\n",
    "_**Don't forget to ask for summaries of your ChatBot session(s) and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatGPT)!**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dab3584",
   "metadata": {},
   "source": [
    "Mostly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aab1be",
   "metadata": {},
   "source": [
    "# Recommended Additional Useful Activities [Optional]\n",
    "\n",
    "The \"Ethical Profesionalism Considerations\" and \"Current Course Project Capability Level\" sections below **are not a part of the required homework assignment**; rather, they are regular weekly guides covering (a) relevant considerations regarding professional and ethical conduct, and (b) the analysis steps for the STA130 course project that are feasible at the current stage of the course\n",
    "\n",
    "<br><details class=\"details-example\"><summary style=\"color:blue\"><u>Ethical Professionalism Considerations</u></summary>\n",
    "\n",
    "### Ethical Professionalism Considerations\n",
    "\n",
    "|![](https://handsondataviz.org/images/14-detect/gdp-baseline-merged-annotated.png)|\n",
    "|-|\n",
    "| |\n",
    "\n",
    "Mark Twain's statment that, \"There are lies, damn lies, and statistics\", reflects a general skepticism towards statistical analysis that has been reinforced through through popular books such as [How to Lie with Statistics](https://en.wikipedia.org/wiki/How_to_Lie_with_Statistics). One place \"statistics\" can be used to decieve is through misuse of charts.  As discussed [here](https://handsondataviz.org/how-to-lie-with-charts.html) and many other places, a primary tactic that can be used to give a misleading impression using a chart is the manipulation of axes or the addition of additional dimensions which distort the meaning of size. **What are the problems with the following graphs?**\n",
    "\n",
    "|![](https://images.ctfassets.net/jicu8fwm4fvs/260tj0wxTFCAlbf4yTzSoy/2b002a49921831ab0dc05415616a1652/blog-misleading-gun-deaths-graph.jpeg)|![](https://photos1.blogger.com/blogger/5757/110/1600/macgraph.jpg)|\n",
    "|-|-|\n",
    "| | |\n",
    "\n",
    "</details>    \n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Current Course Project Capability Level</u></summary>\n",
    "   \n",
    "### Current Course Project Capability Level\n",
    "    \n",
    "**Remember to abide by the [data use agreement](https://static1.squarespace.com/static/60283c2e174c122f8ebe0f39/t/6239c284d610f76fed5a2e69/1647952517436/Data+Use+Agreement+for+the+Canadian+Social+Connection+Survey.pdf) at all times.**\n",
    "\n",
    "Information about the course project is available on the course github repo [here](https://github.com/pointOfive/stat130chat130/tree/main/CP), including a draft [course project specfication](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F23_course_project_specification.ipynb) (subject to change). \n",
    "- The Week 01 HW introduced [STA130F24_CourseProject.ipynb](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F24_CourseProject.ipynb), and the [available variables](https://drive.google.com/file/d/1ISVymGn-WR1lcRs4psIym2N3or5onNBi/view). \n",
    "- Please do not download the [data](https://drive.google.com/file/d/1mbUQlMTrNYA7Ly5eImVRBn16Ehy9Lggo/view) accessible at the bottom of the [CSCS](https://casch.org/cscs) webpage (or the course github repo) multiple times.\n",
    "    \n",
    "At this point in the course you should be able to create a `for` loop to iterate through and provide **visualizations** of some of the interesting columns in the course project data\n",
    "\n",
    "1. Create a `for` loop with a **conditional logic structure** that appropriately controls the kind of visualization that gets made for a given column of data based on its data type\n",
    "\n",
    "*Being able run your code with different subsets (of different types) of columns demonstrates the desirability of the programming design principle of \"polymorphism\" (which means \"many uses\") which states that code is best when it's \"resuable\" for different purposes... such as automatically providing the appropriate visualizations as interest in different variables dynamically changes...* \n",
    "    \n",
    "</details>            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
